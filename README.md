<h2 align="center">👋 Hello! I'm Sixun Dong (Ironieser)</h2>

<p align="center">
  <a href="https://cv.ironieser.cc">🌐 Homepage</a> •
  <a href="https://github.com/Ironieser">💻 GitHub</a> •
  <a href="mailto:sdong46@asu.edu">📧 Email</a> •
  <a href="https://scholar.google.com/citations?user=fuR1FBwAAAAJ&hl=en">🎓 Google Scholar</a>
</p>

<p align="center">
  <img src="https://readme-typing-svg.herokuapp.com/?lines=PhD+Student+%40+ASU;Multimodal+Learning+Researcher;Computer+Vision+%26+VLM+Agent;CVPR+%7C+WACV+%7C+3DV+Author&font=Fira%20Code&center=true&width=600&height=50&color=58a6ff&vCenter=true&size=20">
</p>

---

## 🎓 About Me

I'm a **PhD Student** at [KDD Lab](https://sites.google.com/site/kddlaboratory/), Arizona State University, supervised by Professor [Yanjie Fu](https://faculty.engineering.asu.edu/yanjiefu/). I focus on cutting-edge research in **multimodal learning**, **computer vision**, and **LLM agents**. My work bridges the gap between vision, language, and temporal understanding, with a particular emphasis on weakly supervised learning and efficient model design.

🔬 **Research Interests:**
- **Multimodal Learning**: Vision-Language Models, Cross-modal Understanding
- **Video Understanding**: Temporal Analysis, Action Recognition, Weakly Supervised Learning  
- **Time Series Analysis**: Forecasting with Multimodal Perspectives
- **LLM Agents**: Tool Learning, Feature Transformation, Embodied AI
- **Efficient AI**: Token Pruning, Model Compression, Fast Inference

🎯 **Current Focus**: Developing embodied multimodal agents that can *see, understand, reason, plan, and execute* in open-world scenarios.

---

## 🏆 Research Highlights

### 📚 **Recent Publications**

🔥 **CVPR 2023** - **Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos**  
*First Author* | Video-text alignment without frame-level supervision  
[[Paper](https://arxiv.org/abs/2303.12370)] [[Code](https://github.com/svip-lab/WeakSVR/)]

🔥 **CVPR 2022 Oral** - **TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting**  
*First Author* | Repetitive action counting with transformer architecture  
[[Paper](https://arxiv.org/abs/2204.01018)] [[Code](https://github.com/SvipRepetitionCounting/TransRAC)] [[Dataset](https://svip-lab.github.io/dataset/RepCount_dataset.html)] [[YouTube](https://youtu.be/SFpUS9mHHpk)] [[Bilibili](https://www.bilibili.com/video/BV1B94y1S7oP?share_source=copy_web)]

🔥 **WACV 2024** - **MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning**  
Equipping LLMs with multimodal tool-use capabilities  
[[Paper](https://arxiv.org/abs/2401.10727)] [[Code](https://github.com/MLLM-Tool/MLLM-Tool)]

🔥 **3DV 2024** - **RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation**  
Indoor scene generation with style and shape consistency  
[[Paper](https://arxiv.org/abs/2310.10027)] [[Code](https://github.com/zhao-yiqun/RoomDesigner)]

🔥 **IJCAI 2025** - **Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming**  
LLM agents for intelligent feature engineering  
[[Paper](https://arxiv.org/abs/2504.21304)]

### 🚀 **Current Projects**
- **Efficient Vision-Language Models**: Token pruning strategies for VLM acceleration
- **Time Series Forecasting**: Teaching time series to "see and speak" with visual-textual alignment
- **Agentic Feature Augmentation**: Agent-driven feature transformation with planning and memory

---

## 🛠 Technical Stack

<p align="left">
  <img src="https://skillicons.dev/icons?i=python,pytorch,cpp,linux,git,bash,docker" />
</p>

**Languages & Frameworks:**  
![Python](https://img.shields.io/badge/-Python-3776AB?style=flat-square&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/-PyTorch-EE4C2C?style=flat-square&logo=pytorch&logoColor=white)
![C++](https://img.shields.io/badge/-C++-00599C?style=flat-square&logo=cplusplus&logoColor=white)
![Linux](https://img.shields.io/badge/-Linux-FCC624?style=flat-square&logo=linux&logoColor=black)

**Research Areas:**  
![Computer Vision](https://img.shields.io/badge/-Computer_Vision-FF6B6B?style=flat-square)
![Multimodal Learning](https://img.shields.io/badge/-Multimodal_Learning-4ECDC4?style=flat-square)
![Time Series](https://img.shields.io/badge/-Time_Series-45B7D1?style=flat-square)
![LLM Agents](https://img.shields.io/badge/-LLM_Agents-96CEB4?style=flat-square)

---

## 💼 Professional Experience

### 🔬 **Current Position**
**Research Assistant**  ｜ ASU SCAI ｜ 2024 Aug - Present  
*Advisor: Prof. Yanjie Fu | Focus: Multimodal Learning, Computer Vision, LLM Agent*

### 🏢 **Industry Experience**
**GenAI Research Intern** | Zoom Inc. | May 2025 - Present  
*Efficient Vision-Language Modeling*

**Research Intern (Team Leader)** | DGene | Nov 2023 - Jan 2024  
*Co-Speech Gesture & Head Motion Generation*

**Research Intern (Team Leader)** | Transsion Holdings | Apr 2023 - Aug 2023  
*Audio-Driven Talking Head Video Generation*

---

## 🎓 Education
🎓 **PhD Student** | KDD Lab, Arizona State University | 2024 - Present  

🎓 **M.S. Computer Science** | ShanghaiTech University | 2024  
*[SVIP-Lab](https://svip-lab.github.io/team.html), Advisor: Prof. [Shenghua Gao](https://scholar.google.com/citations?hl=zh-CN&user=fe-1v0MAAAAJ)*

🎓 **B.E. Computer Science (Dual Degree)** | Dalian University of Technology | 2020  
🎓 **B.E. Process Equipment & Control Engineering** | Dalian University of Technology | 2020

---

## 📊 GitHub Stats

<div align="center">
  <img height="180em" src="https://github-readme-stats.vercel.app/api?username=Ironieser&show_icons=true&theme=tokyonight&include_all_commits=true&count_private=true"/>
  <img height="180em" src="https://github-readme-stats.vercel.app/api/top-langs/?username=Ironieser&layout=compact&langs_count=8&theme=tokyonight&hide=jupyter%20notebook,html"/>
</div>

<div align="center">
  <img src="https://github-readme-streak-stats.herokuapp.com/?user=Ironieser&theme=tokyonight" alt="Ironieser" />
</div>

---

## 🤝 Academic Service

**Reviewer for:** CVPR (2023-2025), ICCV (2023-2025), ECCV (2024), ACM MM (2023-2025), ACCV (2024), KDD (2024), NeurIPS 2025, TMM, Neural Networks, TKDD

---

## 🎵 Currently Listening

[![Spotify](https://spotify-github-profile.kittinanx.com/api/view?uid=i2flp3p8h79r8ekhw6zq238of&cover_image=true&theme=compact&show_offline=false&background_color=121212)](https://spotify-github-profile.kittinanx.com/api/view?uid=i2flp3p8h79r8ekhw6zq238of&redirect=true)

---

## 🐍 Contribution Snake

<picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/ironieser/ironieser/output/github-contribution-grid-snake-dark.svg" />
  <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/ironieser/ironieser/output/github-contribution-grid-snake.svg" />
  <img alt="github-snake" src="https://raw.githubusercontent.com/ironieser/ironieser/output/github-contribution-grid-snake.svg" />
</picture>

---

<div align="center">
  
### 💬 Let's Connect!

*"Building the future of multimodal AI, one model at a time."*

[![Email](https://img.shields.io/badge/-Email-EA4335?style=for-the-badge&logo=gmail&logoColor=white)](mailto:sdong46@asu.edu)
[![Homepage](https://img.shields.io/badge/-Homepage-4285F4?style=for-the-badge&logo=google-chrome&logoColor=white)](https://cv.ironieser.cc)
[![Google Scholar](https://img.shields.io/badge/-Google_Scholar-4285F4?style=for-the-badge&logo=google-scholar&logoColor=white)](https://scholar.google.com/citations?user=fuR1FBwAAAAJ&hl=en)

![Profile Views](https://komarev.com/ghpvc/?username=Ironieser&style=for-the-badge&color=brightgreen)

</div>
